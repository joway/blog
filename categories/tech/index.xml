<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tech on Random Thoughts</title><link>https://blog.joway.io/categories/tech/</link><description>Recent content in Tech on Random Thoughts</description><generator>Hugo -- gohugo.io</generator><language>cn</language><lastBuildDate>Mon, 09 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.joway.io/categories/tech/index.xml" rel="self" type="application/rss+xml"/><item><title>重新思考 Go：了解程序在线上是如何运行的</title><link>https://blog.joway.io/posts/golang-rethink-program-in-realworld/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-rethink-program-in-realworld/</guid><description>重新思考 Go 系列：这个系列希望结合工作中在 Go 编程与性能优化中遇到过的问题，探讨 Go 在语言哲学、底层实现和现实需求三者之间关系与矛盾。
前言 过去一段时间，在大量的线上服务 case study 过程中，逐步深入了解了如今的业务 Go 进程是如何在一系列繁杂的基础设施之上运行的。有些表现在意料之中，也有一些出乎意料的发现。
我很喜欢一个叫做「 机械同理心/Mechanical Sympathy 」的概念，大体的意思是你必须深刻了解你的程序/机械装置是在一种怎么样的环境下运行的，设身处地地在这个运行环境下去思考，才能帮助你写出更好的程序，或是解答一些奇怪现象的原因。
本文希望达到的目的，也是构建这份「机械同理心」。
程序能使用多少计算资源？ 对于很多人来说，这个问题似乎非常简单，大多的计算平台都会要求你建立服务的时候就指定好「需要的计算资源」，但这与你的「能使用的计算资源」是两件事情。实际上这个问题的复杂性远远超出大部分人现象，甚至都不是一个常量，也无法使用公式简单计算。抽象地讲，这取决于天时地利人和。
「天时」指的是内核 Cgroups 以及容器调度平台的基本原理和参数设置。这是硬指标，大部分时候也是常量。 「地利」指的是部署的实际物理机的繁忙程度。 「人和」指的是写程序的人得在能够有更多计算资源可用的时候真的让自己程序用上这些计算资源。 接下来我们逐步分节来详细探究这个问题。
被封装的 CPU 谎言 在 Oncall 中常见的一个误解是，研发人员在容器平台上申请了 4 核 CPU 的容器，然后自然而然认为自己的程序最多只能使用 4 个 CPU，于是按照这个计算能力去估算需要的容器数量，以及对自己程序套上这个假设进行参数调优。
上线后，进到容器用 top 一看，各项指标确实是按照 4 核的标准在进行。甚至用 cat /proc/cpuinfo 一看，不多不少刚好看到有 4 个 CPU。。。
但实际上，这一切都只是容器平台为你封装出来的一个美好的假象。之所以要把这个假象做的这么逼真，只是为了让你摆脱编程时的心智负担，顺便再让那些传统的 Linux 观测工具在容器环境中也能正常运行。
但是假象终究不是真相，如果有一天你遇到一些疑难问题或是想要去编写极致性能的代码，你会发现这些封装出来的抽象把你的理解带的有多偏。
我到底能同时使用多少个 CPU ？ 在 K8s 的环境里，CPU 是一个时间单位而非数量。正常情况下，一个拥有 64 核心的机器，你最多能同时使用的理论 CPU 个数是 64 ，即便你创建容器的时候申请的是 4 核。</description></item><item><title>重新思考 Go：Channel 不是「消息队列」</title><link>https://blog.joway.io/posts/golang-rethink-channel/</link><pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-rethink-channel/</guid><description>重新思考 Go 系列：这个系列希望结合工作中在 Go 编程与性能优化中遇到过的问题，探讨 Go 在语言哲学、底层实现和现实需求三者之间关系与矛盾。
Go 语言是一门为实现 CSP 并发模型而设计的语言，这也是它区别于其他语言最大的特色。而为了实现这一点，Go 在语法上就内置了 chan 的数据结构来作为不同协程间通信的载体。
Go 的 channel 提供 input 和 output 两种操作语法。input 一个已经 full 的 channel ，或是 output 一个 empty 的 channel 都会引发整个协程的阻塞。这个协程阻塞性能代价很低，也是协程让渡执行权的主要方法。
ch := make(chan int, 1024) // input ch &amp;lt;- 1 // output &amp;lt;-ch 然而 channel 的实现恰好和进程内消息队列的大部分需求是吻合的，所以这个结构时常被用来作为生产者消费者模型的实现，甚至还作为 channel 的主流应用场景而推广。
但事实上，如果真的把该数据结构用来作为系统内核心链路的生产消费者模型底层实现，一不留神就会遇到雪崩级别的问题，且这些问题都不是简单的代码修改便能解决的。
Input 失败导致阻塞 当 channel 满的时候，&amp;lt;- 操作会导致整个 goroutine 阻塞。显然这并不总是编程者希望的，所以 Go 提供了 select case 的方法来判断 &amp;lt;- 是否成功：
select { case ch &amp;lt;- 1: default: // input failed } 但问题是，当 channel input 失败时，编程者还能怎么做？除非队列的消息是可以被丢弃的，否则我们可能只再去创建一个类似 queue 的结构，将这部分消息缓存下来。但是这个 queue 的结构可能又要和这个 ch 本身的队列顺序处理好并发关系。</description></item><item><title>重新思考 Go：Slice 只是「操作视图」</title><link>https://blog.joway.io/posts/golang-rethink-slice/</link><pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-rethink-slice/</guid><description>重新思考 Go 系列：这个系列希望结合工作中在 Go 编程与性能优化中遇到过的问题，探讨 Go 在语言哲学、底层实现和现实需求三者之间关系与矛盾。
Go 在语法级别上提供了 Slice 类型作为对底层内存的一个「操作视图」:
var sh []any // ==&amp;gt; internal struct of []any type SliceHeader struct { Data uintptr Len int Cap int } 编程者可以使用一些近似 Python 的语法来表达对底层内存边界的控制:
var buf = make([]byte, 1000) tmp := buf[:100] // {Len=100, Cap=1000} tmp = buf[100:] // {Len=900, Cap=900} tmp = buf[100:200:200] // {Len=100, Cap=100} 虽然 Slice 的语法看似简单，但编程者需要时刻记住一点就是 Slice 只是一个对底层内存的「操作视图」，而非底层「内存表示」，Slice 的各种语法本身并不改变底层内存。绝大部分 Slice 有关的编程陷阱根源就在于两者的差异。
Slice 陷阱：持有内存被「放大」 以最简单的从连接中读取一段数据为例，由于我们事先并不知道将会读取到多少数据，所以会预先创建 1024 字节的 buffer ，然而如果此时我们只读取到了 n bytes, n 远小于 1024，并返回了一个 len=n 的 slice，此时这个 slice 的真实内存大小依然是 1024。</description></item><item><title>RPC 漫谈： 连接问题</title><link>https://blog.joway.io/posts/deep-into-rpc-connection/</link><pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/deep-into-rpc-connection/</guid><description>什么是连接 在物理世界并不存在连接这么一说，数据转换为光/电信号后，从一台机器发往另一台机器，中间设备通过信号解析出目的信息来确定如何转发包。我们日常所谓的「连接」纯粹是一个人为抽象的概念，目的是将传输进来的无状态数据通过某个固定字段作为标识，分类为不同有状态会话，从而方便在传输层去实现一些依赖状态的事情。
以 TCP 为例，一开始的三次握手用来在双方确认一个初始序列号（Initial Sequence Numbers，ISN），这个 ISN 标志了一个 TCP 会话，并且这个会话有一个独占的五元组（源 IP 地址，源端口，目的 IP 地址，目的端口，传输层协议）。在物理意义上，一个 TCP 会话等价于通往某一个服务器的相对固定路线（即固定的中间物理设备集合），正是由于这样，我们去针对每个 TCP 会话进行有状态的拥塞控制等操作才是有意义的。
连接的开销 我们常常听到运维会说某台机器连接太多所以出现了服务抖动，大多数时候我们会接受这个说法然后去尝试降低连接数。然而我们很少去思考一个问题，在一个服务连接数过多的时候，机器上的 CPU，内存，网卡往往都有大量的空余资源，为什么还会抖动？维护一个连接的具体开销是哪些？
内存开销：
TCP 协议栈一般由操作系统实现，因为连接是有状态对，所以操作系统需要在内存中保存这个会话信息，这个内存开销每个连接大概 4kb 不到。
文件描述符占用：
在 Linux 视角中，每个连接都是一个文件，都会占用一个文件描述符。文件描述符所占用的内存已经计算在上面的内存开销中，但操作系统为了保护其自身的稳定性和安全性，会限制整个系统内以及每个进程中可被同时打开的最大文件描述符数：
# 机器配置: Linux 1 核 1 GB $ cat /proc/sys/fs/file-max 97292 $ ulimit -n 1024 上面的设置表示整个操作系统最多能同时打开 97292 个文件，每个进程最多同时打开 1024 个文件。
严格来说文件描述符根本算不上是一个资源，真正的资源是内存。如果你有明确的需要，完全可以通过设置一个极大值，让所有应用绕开这个限制。
线程开销：
有一些较老的 Server 实现采用的还是为每个连接独占（新建或从连接池中获取）一个线程提供服务的方式，对于这类服务来说，除了连接本身占用的外，还有线程的固定内存开销：
# 机器配置: Linux 1 核 1 GB # 操作系统最大线程数 $ cat /proc/sys/kernel/threads-max 7619 # 操作系统单进程最大线程数，undef 表示未限制 $ cat /usr/include/bits/local_lim.</description></item><item><title>RPC 漫谈：序列化问题</title><link>https://blog.joway.io/posts/deep-into-rpc-serialization/</link><pubDate>Fri, 30 Apr 2021 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/deep-into-rpc-serialization/</guid><description>何为序列 对于计算机而言，一切数据皆为二进制序列。但编程人员为了以人类可读可控的形式处理这些二进制数据，于是发明了数据类型和结构的概念，数据类型用以标注一段二进制数据的解析方式，数据结构用以标注多段(连续/不连续)二进制数据的组织方式。
例如以下程序结构体：
type User struct { Name string Email string } Name 和 Email 分别表示两块独立(或连续，或不连续)的内存空间（数据），结构体变量本身也有一个内存地址。
在单进程中，我们可以通过分享该结构体地址来交换数据。但如果要将该数据通过网络传输给其他机器的进程，我们需要现将该 User 对象中不同的内存空间，编码成一段连续二进制表示，此即为「序列化」。而对端机器收到了该二进制流以后，还需要能够认出该数据为 User 对象，解析为程序内部表示，此即为「反序列化」。
序列化和反序列化，就是将同一份数据，在人的视角和机器的视角之间相互转换。
序列化过程 定义接口描述（IDL） 为了传递数据描述信息，同时也为了多人协作的规范，我们一般会将描述信息定义在一个由 IDL(Interface Description Languages) 编写的定义文件中，例如下面这个 Protobuf 的 IDL 定义：
message User { string name = 1; string email = 2; } 生成 Stub 代码 无论使用什么样的序列化方法，最终的目的是要变成程序中里的一个对象，虽然序列化方法往往是语言无关的，但这段将内存空间与程序内部表示（如 struct/class）相绑定的过程却是语言相关的，所以很多序列化库才会需要提供对应的编译器，将 IDL 文件编译成目标语言的 Stub 代码。
Stub 代码内容一般分为两块：
类型结构体生成（即目标语言的 Struct[Golang]/Class[Java] ） 序列化/反序列化代码生成（将二进制流与目标语言结构体相转换） 下面是一段 Thrift 生成的的序列化 Stub 代码：
type User struct { Name string `thrift:&amp;#34;name,1&amp;#34; db:&amp;#34;name&amp;#34; json:&amp;#34;name&amp;#34;` Email string `thrift:&amp;#34;email,2&amp;#34; db:&amp;#34;email&amp;#34; json:&amp;#34;email&amp;#34;` } //写入 User struct func (p *User) Write(oprot thrift.</description></item><item><title>RPC 漫谈： 限流问题</title><link>https://blog.joway.io/posts/deep-into-rpc-ratelimiter/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/deep-into-rpc-ratelimiter/</guid><description>微服务之间的 RPC 调用往往会使用到限流功能，但是很多时候我们都是用很简单的限流策略，亦或是工程师拍脑袋定一个限流值。
这篇文章主要讨论在 RPC 限流中，当前存在的问题和可能的解决思路。
为什么需要限流 避免连锁崩溃 一个服务即便进行过压测，但当真实运行到线上时，其收到的请求流量以及能够负载的流量是不固定的，如果服务自身没有一个自我保护机制，当流量超过预计的负载后，会将这部分负载传递给该服务的下游，造成连锁反应甚至雪崩。
提供可靠的响应时间 服务调用方一般都设有超时时间，如果一个服务由于拥塞，导致响应时间都处于超时状态，那么即便服务最终正确提供了响应，对于 Client 来说也完全没有意义。
一个服务对于调用方提供的承诺既包含了响应的结果，也包含了响应的时间。限流能够让服务自身通过主动丢弃负载能力外的流量，以达到在额定负载能力下，依然能够维持有效的响应效率。
传统方案 漏斗 优点：
能够强制限制出口流量速率 缺点：
无法适应突发性流量 令牌桶 优点：
在统计上维持一个特定的平均速度 在局部允许短暂突发性流量通过 存在的问题 在两类传统方案中，都需要去指定一个固定值用以标明服务所能够接受的负载，但在现代的微服务架构中，一个服务的负载能力往往是会不断变化的，有以下几个常见的原因：
随着新增代码性能变化而变化 随着服务依赖的下游性能变化而变化 随着服务部署的机器(CPU/磁盘)性能变化而变化 随着服务部署的节点数变化而变化 随着业务需求变化而变化 随着一天时间段变化而变化 通过人工声明一个服务允许的负载值，即便这个值是维护在配置中心可以动态变化，但依然是不可持续维护的，况且该值具体设置多少也极度依赖于人的个人经验和判断。甚至人自身的小心思也会被带入到这个值的选择中，例如 Server 会保守估计自己的能力，Client 会过多声明自己的需求，长期以往会导致最终的人为设定值脱离了实际情况。
什么是服务负载 当我们向一个服务发起请求时，我们关心的无外乎两点：
服务能够支撑的同时并发请求数 服务的响应时间 并发请求数 对于 Server 而言，有几个指标常常容易搞混：
当前连接数 当前接受的请求数 当前正在并发处理的请求数 QPS 连接数和请求数是 1:N 的关系。在现代 Server 的实现中，连接本身消耗的服务器资源已经非常少了（例如 Java Netty 实现，Go Net 实现等），而且一般对内网的服务而言，多路复用时，请求数变多也并不一定会导致连接数变多。
有些 Server 出于流量整形角度的考虑，并不一定会在收到请求以后，立马交给 Server 响应函数处理，而是先加入到一个队列中，等 Server 有闲置 Worker 后再去执行。所以这里就存在两类请求：接受的请求与正在处理的请求。
而 QPS 是一个统计指标，仅仅只表示每秒通过了多少请求。</description></item><item><title>Pond: Golang 通用对象池</title><link>https://blog.joway.io/posts/golang-common-pool/</link><pubDate>Sat, 23 Jan 2021 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-common-pool/</guid><description>为什么需要通用对象池 在实际生产环境中，我们经常会遇到需要多线程共享同一堆对象的场景，例如常见的RPC、数据库连接池，大内存对象池，以及线程池等。这些池化对象的需求其实有很多重叠的地方，主要分以下几个方面：
基础设置： 最大容量 创建/校验/删除对象的回调方法 申请对象时，已满时是否阻塞 多重驱逐策略： 驱逐闲置对象 驱逐无效对象 预创建对象 为避免重复编码，所以设计了 Pond 这样一个能够支撑多种使用场景的通用对象池。另外，在 Pond 的基础上，还实现了 Goroutine 池库： Hive。
使用方式 //example type conn struct { addr string } func main() { ctx := context.Background() cfg := pond.NewDefaultConfig() cfg.MinIdle = 1 cfg.ObjectCreateFactory = func(ctx context.Context) (interface{}, error) { return &amp;amp;conn{addr: &amp;#34;127.0.0.1&amp;#34;}, nil } cfg.ObjectValidateFactory = func(ctx context.Context, object interface{}) bool { c := object.(*conn) return c.addr != &amp;#34;&amp;#34; } cfg.ObjectDestroyFactory = func(ctx context.</description></item><item><title>Golang for-range 内部实现</title><link>https://blog.joway.io/posts/golang-range-internal/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-range-internal/</guid><description>最近在写一个编解码的功能时发现使用 Golang for-range 会存在很大的性能问题。
假设我们现在有一个 Data 类型表示一个数据包，我们从网络中获取到了 [1024]Data 个数据包，此时我们需要对其进行遍历操作。一般我们会使用 for-i++ 或者 for-range 两种方式遍历，如下代码：
type Data [256]byte func BenchmarkForStruct(b *testing.B) { var items [1024]Data var result Data for i := 0; i &amp;lt; b.N; i++ { for k := 0; k &amp;lt; len(items); k++ { result = items[k] } } _ = result } func BenchmarkRangeStruct(b *testing.B) { var items [1024]Data var result Data for i := 0; i &amp;lt; b.N; i++ { for _, item := range items { result = item } } _ = result } 输出结果：</description></item><item><title>Golang Interface 内部实现</title><link>https://blog.joway.io/posts/golang-interface-internal/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-interface-internal/</guid><description>最近遇到一个由于 Golang Interface 底层实现，引发的线上 panic 问题，虽然根源在于赋值操作没有保护起来，却意外地发现了关于 interface 的一些有意思的底层细节。
假设我们现在有以下定义：
type Interface interface { Run() } type Implement struct { n int } func (i *Implement) Run() { fmt.Printf(i.n) } 对于使用者而言，一个变量无论是 Interface 类型或是 *Implement 类型，差别都不大。
func main() { var i Interface fmt.Printf(&amp;#34;%T\n&amp;#34;, i) //&amp;lt;nil&amp;gt; i = &amp;amp;Implement{n: 1} fmt.Printf(&amp;#34;%T\n&amp;#34;, i) //*main.Implement var p *Implement fmt.Printf(&amp;#34;%T\n&amp;#34;, p) //*main.Implement p = &amp;amp;Implement{n: 1} fmt.Printf(&amp;#34;%T\n&amp;#34;, p) //*main.Implement } 如果现在有这么一段代码：
func check(i Interface) { if i == nil { return } impl := i.</description></item><item><title>Golang rand 库锁竞争优化</title><link>https://blog.joway.io/posts/golang-fastrand/</link><pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-fastrand/</guid><description>背景 最近在实现一个随机负载均衡器的时候发现一个问题，在高并发的情况下，官方标准库 rand.Intn() 性能会急剧下降。翻了下实现以后才发现它内部居然是全局共享了同一个 globalRand 对象。
一段测试代码：
func BenchmarkGlobalRand(b *testing.B) { b.RunParallel(func(pb *testing.PB) { for pb.Next() { rand.Intn(100) } }) } func BenchmarkCustomRand(b *testing.B) { b.RunParallel(func(pb *testing.PB) { rd := rand.New(rand.NewSource(time.Now().Unix())) for pb.Next() { rd.Intn(100) } }) } 输出：
BenchmarkGlobalRand BenchmarkGlobalRand-8 18075486 66.1 ns/op BenchmarkCustomRand BenchmarkCustomRand-8 423686118 2.38 ns/op 解决思路 最理想对情况是可以在每个 goroutine 内创建一个私有的 rand.Rand 对象，从而实现真正的无锁。
但在很多其他场景下，我们并不能直接控制调用我们的 goroutine，又或者 goroutine 数量过多以至于无法承受这部分内存成本。
此时的一个思路是使用 sync.Pool 来为 rand.Source 创建一个池，当多线程并发读写时，优先从自己当前 P 中的 poolLocal 中获取，从而减少锁的竞争。同时由于只是用 pool 扩展了原生的 rngSource 对象，所以可以兼容其 rand.</description></item><item><title>分布式文件系统的演化</title><link>https://blog.joway.io/posts/deep-into-distributed-filesystem/</link><pubDate>Sun, 14 Jun 2020 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/deep-into-distributed-filesystem/</guid><description>文件系统是操作系统 IO 栈里非常重要的一个中间层，其存在的意义是为了让上层应用程序有一层更加符合人类直觉的抽象来进行文档的读写，而无需考虑底层存储上的细节。
本地文件系统 在讨论分布式文件系统前，我们先来回顾下本地文件系统的组成。
存储结构 在前面一张图里，我们能够看到文件系统直接和通用块层进行交互，无论底层存储介质是磁盘还是 SSD，都被该层抽象为 Block 的概念。文件系统在初始化时，会先在挂载的块存储上的第一个位置创建一个 Super Block：
上图右边部分就是一块完整的存储，可以将其想象成一个数组。
Super Block 中存储了该文件系统的信息，其组成部分如下：
Magic: MAGIC_NUMBER or 0xf0f03410 ，用来告诉操作系统该磁盘是否已经拥有了一个有效的文件系统。 Blocks: blocks 总数 InodeBlocks: 其中属于 inode 的 block 数 Inodes: 在 InodeBlocks 中存在多少个 inodes 由于这里的 Blocks 总数、InodeBlocks 总数、每个 Block 的大小在文件系统创建时就已经固定，所以一般来说一个文件系统能够创建的文件数量在一开始就已经固定了。
Linux 中每个文件都拥有一个唯一的 Inode，其结构如下：
inode 上半部分的 meta data 很容易理解，下半部分的 block 指针的含义分别为：
direct block pointer: 直接指向 data block 的地址 indirect block: 指向 direct block double indirect block: 指向 indirect block triple indirect block: 指向 double indirect block 由于一个 inode 大小固定，所以这里的 block pointers 数量也是固定的，进而单个文件能够占用的 data block 大小也是固定的，所以一般文件系统都会存在最大支持的文件大小。</description></item><item><title>从动物森友会聊主机游戏联机机制</title><link>https://blog.joway.io/posts/how-animal-crossing-online-work/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/how-animal-crossing-online-work/</guid><description>最近在玩动物森友会的时候时常会遇到一些迷之联机问题，在网上一番搜索，发现大家的答案都趋于用玄学来解释，于是便有了兴致想在原理上搞懂这些问题产生的根源以及动森这款游戏的一些联机设定背后的技术原因。
事先声明，本人并不从事游戏行业亦非主机游戏长期玩家，如有纰漏或其他角度的补充，欢迎在评论区告知。
游戏是如何同步的 我们首先来看看一般游戏是如何来做同步的。
想象两个独立房间里分别有甲、乙两个玩家，他们要远程下一局象棋。他们每下一步前都需要先获知到当前棋盘的情况，此时能够有两种实现方式。
第一种叫做锁步同步，原理是玩家每操作一步就通知给另外一个玩家，彼此同步当前的操作序列，通过这些有时序的操作，就能够计算出当前棋局的状态。但它不允许中间丢失任何一步的信息，否则就会出现非常大的计算偏差。
第二种叫做状态同步，顾名思义是玩家每操作一步，就同步整个棋盘的状态。这种方式可以容忍中间某些状态丢失，最终得到的状态依旧还是一致的。
在实际实践中，针对那种玩家操作非常高频的游戏会更多使用锁步同步，例如王者荣耀。而对于那些卡牌类游戏更偏向于直接用状态同步。
游戏是如何联机的 通信架构 无论是上述哪种同步方式，我们都需要通过网络在多个主机间交换数据。我们现在将场景转换成甲、乙、丙三个人一起下跳棋。为保证三个人最终得到的游戏状态都是一致的，我们往往需要有一台 Host 主机来作为权威主机，其他主机只能通过权威主机下发的数据(状态/操作序列)来更新自己本地的游戏数据。
在这里我们假设甲来做「Host」，乙、丙每操作一步，都需要先发送给甲确认，无误后再发送该操作被确认的信息给乙、丙，乙、丙此时才能够认为操作成功并将画面更新到最新的状态。甲主机上在任意时刻都存有当前游戏的真正状态，其他主机只是在 follow 甲主机的状态以更新自己的游戏画面。
在上述模式下，由于甲主机既要作为游戏主机，又要作为状态同步的主机，当联机用户数一多，甲主机就会不堪重负，出现所谓的「炸机/炸岛」现象。另外，这种模式会需要甲主机一直存活，只能作为短时间内的伺服方案。所以有些游戏会引入一个外部自建/官方的服务器来承担这个状态同步的功能，例如我的世界。但究其原理是一模一样的。
NAT 穿透 在了解完上面的基础知识后，我们能够发现，在不考虑外部服务器的情况下，我们会对玩家主机间的网络有以下几点要求：
甲能够向乙、丙发送数据 乙、丙能够向甲发送数据 乙、丙之间不需要有网络联通保障 虽然上述要求看起来很容易，但是由于现在网络运营商都会不同程度地使用 NAT 技术，所以导致要让两台家用主机建立双向通信并不是一件非常容易的事情。
家用网络一般有四种不同的 NAT 类型：
Full-cone NAT：
Once an internal address (iAddr:iPort) is mapped to an external address (eAddr:ePort), any packets from iAddr:iPort are sent through eAddr:ePort. Any external host can send packets to iAddr:iPort by sending packets to eAddr:ePort. (Address)-restricted-cone NAT：
Once an internal address (iAddr:iPort) is mapped to an external address (eAddr:ePort), any packets from iAddr:iPort are sent through eAddr:ePort.</description></item><item><title>NodeJS 内存泄漏检测与定位</title><link>https://blog.joway.io/posts/nodejs-debug/</link><pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/nodejs-debug/</guid><description>最近解决了一个 Node.JS 应用内存泄漏 Bug，顺便学会了用 Chrome DevTools 去看 heapdump 文件。这里做一些简单的记录。
如何「优雅地」获得 heapdump 文件 由于我们所有应用都是以容器部署的，所以要去获得某个容器内的文件，并拷贝到本地难度还是比较大，也非常麻烦。考虑到调试时或许会需要下载非常多次的 snapshot 文件，建议可以包下 heapdump 库，做成一个接口，把文件 dump 之后再传输给客户端，这样一劳永逸。
需要小心的是，在 heapdump 的时候内存很容易翻倍，所以当内存超过 500 MB的时候如果去 heapdump 非常容易导致 OOM 从而 Crash。
如何检测内存泄漏 检查内存泄漏有两种方法，一种是针对比较大的内存泄漏可以直接观察内存是否一直在稳步上升。如果是一些小的泄漏使得内存上升变化并不非常明显的话，可以通过对比不同时间的 heapdump 文件。
有时候内存上升也可能是因为本身访问量就在上升，所以需要两者对比着分析。
Heapdump 文件对比 通过下载两份间隔一段时间(几分钟)的 heapdump 文件，打开 Chrome DevTools，进入 Memory Tab，选择 Load。选中其中时间更近的 heapdump ，并选择 Comparison，比较对象是老的那份 heapdump：
此时可以选择按 Delta 排序，可以看到两个时间点增加了哪些新的对象。
如图可以看到 string 和 Object 的 Delta 是差不多的，所以可以比较确定是由于 Object 里产生了大量一些 string 对象导致的数量增多，但并不一定能够100%确定是内存泄漏，也可能是正常业务波动。此时需要再拉新的一个时间点的 heapdump 文件再来对比，如果一直在增加，那么内存泄漏的可能性就非常大了。
如何定位内存泄漏 首先依旧是拿到 heapdump 文件，并在 Chrome 中打开。</description></item><item><title>设计实现高性能本地内存缓存</title><link>https://blog.joway.io/posts/modern-memory-cache/</link><pubDate>Sun, 10 Nov 2019 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/modern-memory-cache/</guid><description>本地内存缓存是一个在基础软件架构中非常常见的基础设施，也正因其过于常见，以至于平时很少去思考它是如何实现的。在尚未设计缓存系统前，完全没想到原来要需要考虑如此多复杂的事情。本文将由浅入深介绍如何设计一个现代的高性能内存缓存系统。
什么时候需要本地内存缓存 在大部分业务系统中，都会使用诸如 Redis、Memcached 等远程缓存，一方面可以避免自身进程内存占用过大而导致的 OOM 或 GC 问题，另一方面也可以实现多个进程共享同一份一致的缓存数据。但对于某些底层服务（例如数据库服务），远程缓存的网络延迟是不可接受的，这就势必需要引入本地内存缓存。
本地内存缓存的特点 本地内存缓存可被视作一个基于本地内存的 「KeyValue 数据库」。但相比较于传统数据库而言，它对一致性的要求十分宽松：
对于更新与删除的操作，需要保证强一致性 对于插入操作可以容忍少量丢失 对于读取操作可以容忍少量 Miss 与磁盘数据库的另一个不同之处在于，磁盘数据库的设计有一个前提假设是磁盘是可以随需要而不断扩容的，倘若一个磁盘数据库因磁盘占满而崩溃主要责任是在使用方。而内存缓存则没有这么宽容的假设可以建立，它必须考虑到内存是昂贵且有限的这一事实。
除此之外，由于本地内存缓存处于业务进程当中，所以其需要考虑更多业务向的问题，比如：
由于自身大量老生代的内存占用，是否会对所处进程产生 GC 问题。 当多线程场景下，如何同时解决线程安全、数据竞争、高吞吐等问题。 需要能够适应一些非随机的访问统计规律，例如 Zipf。 综上所述，我们可以归纳出对一个优秀的本地内存缓存系统的要求：
线程安全 高吞吐 高命中率 支持内存限制 实现路径 在实现一个完整的缓存系统前，我们需要将目标一步步拆解。
首先为了实现缓存逻辑，我们必须有一个类 Map 的 KeyValue 数据结构，同时它必须是线程安全的。为了支持内存限制，我们必须要能够驱逐一些 key，所以需要实现一个驱逐器。为了实现驱逐的同时维持高命中率，我们还需要告诉驱逐器每个 key 的访问记录，让它能够从中分析出哪些 key 可以被驱逐。综上分析，我们可以整理出一个大概的 Roadmap：
实现一个线程安全的 Map 数据结构：存储缓存内容 实现一个访问记录队列：存储访问记录 实现一个驱逐器：管理缓存内容 本文所有代码均使用 Golang 编写。
线程安全的 Map 简易的 Map cache := map[string]string{} cache[&amp;quot;a&amp;quot;] = &amp;quot;b&amp;quot; 在 key 数量固定且极少的情况下，我们一般会用原生 Map 直接实现一个最简单缓存。但 Golang 原生 Map 并不是线程安全的，当多个 goroutine 同时读写该对象时，会发生冲突。</description></item><item><title>Linux I/O 栈浅析</title><link>https://blog.joway.io/posts/linux-io-stack/</link><pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/linux-io-stack/</guid><description>在 Linux 中，所有外部资源都以文件形式作为一个抽象视图，并提供一套统一的接口给应用程序调用。本文将以宏观视角试图阐述 Linux 中关于文件 IO 的整个调用脉络。
VFS 在 Linux 中，所有 IO 都必须先经由 VFS 层进行转发。通过 VFS 将包括磁盘、网络 Socket、打印机、管道等资源全部封装成统一的接口。
基础结构 VFS 自顶向下使用四个数据结构来描述文件：
file: 存放一个文件对象的信息。 struct file { union { struct llist_node fu_llist; struct rcu_head fu_rcuhead; } f_u; struct path f_path; struct inode *f_inode; /* cached value */ const struct file_operations *f_op; struct mutex f_pos_lock; loff_t f_pos; } dentry: 存放目录项和其下的文件链接信息。 struct dentry { unsigned int d_flags; seqcount_t d_seq; struct hlist_bl_node d_hash; /* 哈希链表 */ struct dentry *d_parent; /* 父目录项 */ struct qstr d_name; /* 目录名 */ struct inode *d_inode; /* 对应的索引节点 */ unsigned char d_iname[DNAME_INLINE_LEN]; /* small names */ struct lockref d_lockref; /* per-dentry lock and refcount */ const struct dentry_operations *d_op; /* dentry操作 */ struct super_block *d_sb; /* 文件的超级块对象 */ unsigned long d_time; void *d_fsdata; struct list_head d_lru; /* LRU list */ struct list_head d_child; /* child of parent list */ struct list_head d_subdirs; /* our children */ union { struct hlist_node d_alias; /* inode alias list */ struct rcu_head d_rcu; } d_u; } inode: 索引节点对象，存在具体文件的一般信息，文件系统中的文件的唯一标识。 struct inode { struct hlist_node i_hash; /* 散列表，用于快速查找inode */ struct list_head i_list; /* 相同状态索引节点链表 */ struct list_head i_sb_list; /* 文件系统中所有节点链表 */ struct list_head i_dentry; /* 目录项链表 */ unsigned long i_ino; /* 节点号 */ atomic_t i_count; /* 引用计数 */ unsigned int i_nlink; /* 硬链接数 */ uid_t i_uid; /* 使用者id */ gid_t i_gid; /* 使用组id */ struct timespec i_atime; /* 最后访问时间 */ struct timespec i_mtime; /* 最后修改时间 */ struct timespec i_ctime; /* 最后改变时间 */ const struct inode_operations *i_op; /* 索引节点操作函数 */ const struct file_operations *i_fop; /* 缺省的索引节点操作 */ struct super_block *i_sb; /* 相关的超级块 */ struct address_space *i_mapping; /* 相关的地址映射 */ struct address_space i_data; /* 设备地址映射 */ unsigned int i_flags; /* 文件系统标志 */ void *i_private; /* fs 私有指针 */ unsigned long i_state; }; superblock: 超级块对象，记录该文件系统的整体信息。在文件系统安装时建立，在文件系统卸载时删除。 链接 硬链接 VS 软链接:</description></item><item><title>SSD 背后的奥秘</title><link>https://blog.joway.io/posts/ssd-notes/</link><pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/ssd-notes/</guid><description>过去很长一段时间里，我对 SSD 的了解仅限于其和 HDD 的区别和一个标签化的「速度快」认知，至于其什么时候快，为什么快却鲜有了解。直到最近开始研究数据库时，发现数据库设计和存储发展和特性紧密联系，不可分割，于是才开始回过头关注起 SSD 的结构和原理，猛然发现之前关于 SSD 有许多非常错误的认识。
SSD 的基本结构 在了解 SSD 性质前，简单回顾下 SSD 的基本结构组成，下面是两张 SSD 的架构图：
其中，SSD Controller 用以执行耗损平衡、垃圾回收、坏快映射、错误检查纠正、加密等功能。相比与 HDD，它的工作非常繁重，而这些工作极大地影响了 SSD 的性能表现，后文会详细谈到。SSD 内部的闪存（Flash）由一个个闪存单元组成，每个闪存单元都有一个寿命，超过寿命将导致坏块。常见有三种闪存单元类型：
SLC：每个单元 1 比特 MLC：每个单元 2 比特 TLC：每个单元 3 比特 每种 NAND 类型有不同的性能和寿命表现，如下表：
闪存单元内部由一个个 Block 组成，每个 Block 由多个 Page 组成。
对于闪存的访问有以下限制：
读写数据只能以 Page 为单位 擦除数据只能以 Block 为单位 每个 Page 大小一般为 2 KB 到 16 KB，这意味着使用 SSD 时，哪怕读或写 1 Byte 的数据，SSD 依旧会访问整个 Page。
此外，SSD 并不允许覆盖的操作，当原本 Page 中已有数据时，只能先删除再写入，为了加快写入速度，一般 SSD 修改数据时，会先写入其他空闲页，再将原始页标记为 stale ，直到最终被垃圾回收擦除。</description></item><item><title>什么是真正的编程能力</title><link>https://blog.joway.io/posts/what-is-the-programming-ability/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/what-is-the-programming-ability/</guid><description>四年前在知乎提过一个问题，「什么是真正的编程能力」，当时这个问题获得了许多业内前辈的热诚回答。时过境迁，前段时间和朋友谈起最初使用知乎的经历，这才又想起当年年少无知提的这个问题，多年以后去一一回顾当时大家的回答，对每句话都有了更为深刻的理解和感触。
如果我在今天去回答四年前自己提出的问题，我会如回答里的诸多其他工程师一样，将编程能力定义为「解决问题的能力」。在不同职业阶段对这话的理解可能会大有不同。在四年前，我会认为这句话的意思是「通过熟练且优异的编程技巧进行解决问题的能力」，着重点依旧在编程本身。但如今我却「背叛」了编程，或者说我所认为的编程能力不应当局限于「狭义的编程」本身。
如果我们抛开这些年上层编程技术日新月异的发展，从编程的本质来看，编程的实质无非是用一套形式化语言去定义问题和描述解决问题的步骤。一个文学作家亦可以用自然语言实现他自己的编程工作，只不过最终的编译器和运行环境是人类的大脑 —— 过去几千年的数学家就是如此工作，并在自然语言基础上发明了更清晰严格的数学形式语言。计算机编程较之人脑的优越性在于更为可靠和高效的运行环境，使得数学得以脱离人类大脑的局限，完成更为复杂的工作。
然而现实生活远比数学家想象的世界要复杂的多，现实问题在被形式化成为计算机语言前，往往要先经过人类使用自然语言进行反复沟通和辩论的过程，最终才会达成一致并被批准以落实成计算机语言。甚至于事实上相当一部分工作在自然语言阶段就足以被妥善解决，完全没有使用计算机的必要。许多人最初认为的编程能力，仅限于最后一步把自然语言的需求翻译成计算机语言而已，但事实上，一个只了解计算机语言编程的人或许是个不错的 Coder，但很难被称之为是一个合格的工程师，也不可能开发出伟大的 Software。
如果一个工程师在编程活动中被一个他所不熟悉的语言所编写的软件所阻挡，最工程师的方法自然是学会这门语言然后解决这个软件的问题。以此类推当一个工程师在现实生活中被一群完全与他处于不同话语体系和思维体系的人所阻挡，最工程师的方法同样是学会他们的语言，使用他们的方式去描述问题、解决问题。
语言亦或思想都是编程的工具，而编程又是为解决问题而创造的工具。我们经常看到一些多年某门计算机语言的从业者对于计算机的认知被牢牢锁死在这门语言框定的世界里，与之相似的是有更大一部分人对于世界的认知被锁死在计算机语言所框定的世界里。这既是从业者的悲哀也是计算机科学的悲哀。
以上所述，如果要我将「解决问题的能力」展开成一个更为清晰的定义，我会认为，真正的编程能力是能够在这个复杂的世界里，针对不同语言体系的人理解并使用不同的描述形式，编写出一个用以清晰界定问题并描述出解决方法的「程序」的能力。
希望再过四年我再来回顾这个问题时，能够有更为别致的感受。</description></item><item><title>命令行里的设计艺术</title><link>https://blog.joway.io/posts/the-art-of-cli-design/</link><pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/the-art-of-cli-design/</guid><description>在谈论手机 App 或是网页时，我们总会谈到交互设计，但倘若涉及到面向专业用户的 CLI(Command-line interface)领域，很少有人会将它与用户交互相联系。甚至在很多人的大脑里，已经把 CLI 和「难用」画上了等号，更有甚者认为 CLI 的难用才体现了其 「专业性」。
与此同时，「RTFM」(Read The Fucking Manual) 作为一个梗在工程师群体广泛流传，也让许多作者对于其不好用的 CLI 有了一个天然的借口。
虽然我们都知道阅读文档是一个好习惯，但恐怕大部分人在用一个新命令前都不会去仔细阅读完它的文档手册，就算你今天读了，你不可能永远记得，也不可能每次记忆模凌两可的时候都去重读一遍。所以可想而知一个高度依赖文档用户才能够正常使用的 CLI 不是一个合格的 CLI。
以下是我对于命令行设计的一些个人观点与观察，所涉并不一定广泛和正确，仅作为抛砖引玉，也欢迎在评论里提出你的看法。
遵循约定俗成 例如 mv, cp, ln 这些命令都遵循 [action] [source_file] [target_file] 的格式，这不一定很有逻辑，但既然都已经约定俗成了，如果你违犯这个顺序要倒过来，其实是一件蛮缺德的事情。为什么说「缺德」？因为一个用过了你的命令的人，他很容易开始怀疑是不是还有别的命令行打破了这个约定，他会对其它命令也不放心，最后他甚至会忘了真正被广泛采用的约定用法是什么了，导致每次用类似语法的命令都胆颤心惊，这点我深有体会 😔。
所以如果你的命令有类似的约定俗成可以遵守，你应该遵守业内的这种约定，这是一种职业道德。
一致的命令组织结构 对所有和用户打交道的产品来说，「一致性」是天条。我遇到过两种被广泛采用的组织风格:
$ [cmd] [module] [flags] [args] $ [cmd] [action] [flags] [args] 无论是按模块划分还是按行为划分本质思路其实都是一样的，有些人会认为项目大了会难以遵守这套规范，但即便是目前规模已经非常庞大的 kubectl，它依旧坚持以 [cmd] [action] [flags] [args] 为基础的设计准则。在有些较为复杂的地方，它可以用 subcmd 来进一步向外部隐藏复杂性 [cmd] [subcmd] [action] [flags] [args]，例如：
$ kubectl config [action] [flags] [args] 我几乎每天都会用到 kubectl，但我的确很少去看它的文档，甚至我都想不到我是怎么就会使用它的。越是在复杂的项目面前，这种一致性带来的好处越明显。
而一个典型的反面教材是 Git。它是我用过的最复杂同时又是最混乱的 CLI，以一般人最常用的分支操作举例：</description></item><item><title>从程序到人 —— 情头配对助手的前世今生</title><link>https://blog.joway.io/posts/avatar-matcher/</link><pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/avatar-matcher/</guid><description>&lt;h2 id="背景">背景&lt;/h2>
&lt;p>情头(情侣头像)一般指成双成对的头像，可以是真人照片，也可以是卡通人物等图片。衍生出去还有闺密头像和基友头像等等。&lt;/p>
&lt;p>&lt;img src="https://ik.imagekit.io/elsetech/blog/images/old-blog/1538015444.png?tr=w-1024">&lt;/p>
&lt;p>社交媒体上有一个非常令人费解的现象是，如果你去即刻、豆瓣、百度贴吧、微博，会发现有大量的人在上面贴了一个头像，然后寻找和它匹配的另一半头像，例如这样:&lt;/p>
&lt;p>&lt;img src="https://ik.imagekit.io/elsetech/blog/images/old-blog/1538016090.png?tr=w-1024">&lt;/p>
&lt;p>市面上的情侣头像大多是一些社区里的大佬自己制作出来的，然后发到社区里，再慢慢流传开来。这种传播方式导致了很多情头在传播时候早就被拆散了。很多人可能只找到了其中一个，但是想要找到和它配对的另一半。&lt;/p>
&lt;p>还有一个问题是，当你看到一个头像时，没有人能够确定这个头像是否在制作时候就有另外一半。&lt;/p>
&lt;p>一些程序员朋友最早看到这个问题时，总会天真地想去搜索引擎里识图一下就行了。但这里有两个非常有趣的问题:&lt;/p>
&lt;p>第一个是目前搜索引擎的识别图片能力其实并不强，比如以 All in AI 著称的百度:&lt;/p>
&lt;p>&lt;img src="https://ik.imagekit.io/elsetech/blog/images/old-blog/1538016451.png?tr=w-1024">&lt;/p>
&lt;p>这还是在图片是原图的情况下，经常一些小朋友会在情头上自己二次创作，比如裁剪，比如压缩，比如贴上什么爱心。那样基本上识图就废了。&lt;/p>
&lt;p>第二个问题更加有趣，情头的特点就是大家都在用，所以在最理想的情况下，即便搜索引擎能够识别出所有有这个头像的网站，出来的结果也并没有什么用处，无非是找到了也在用这个头像的别的网站的用户。&lt;/p>
&lt;p>&lt;img src="https://ik.imagekit.io/elsetech/blog/images/old-blog/1538016570.png?tr=w-1024">&lt;/p>
&lt;p>只有一种情况是真正能够帮助到寻找情侣头像这件事情的，那就是搜索引擎出来的结果里是专门搜集匹配好的情头的站点。那样姑且用户还能点进网页里去找到另外一半。&lt;/p>
&lt;p>从上述阐述里不难发现，指望一个未成年小女生使用一系列高级互联网骚操作找一个头像是有多么不现实，何况技术上可行性还很低。&lt;/p></description></item><item><title>即刻多端实时通信实践</title><link>https://blog.joway.io/posts/socket-io/</link><pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/socket-io/</guid><description>背景 jike-io 是即刻基于 socket.io 构建的一个实时通信基础设施。目前客户端上的所有实时通信服务都是建立在其基础上，涵盖了私信、消息通知、用户反馈、活动页小游戏等诸多组件。
在目前即刻的实时通信设计里，我们的实时通信只是为了让服务端主动推送消息给客户端，客户端不会主动通过 websocket 发送消息。由于几乎我们所有需要发送消息的请求都会有一定业务逻辑在，而这个业务逻辑我们并不希望 websocket 连接层(jike-io)去处理，所以我们仍旧采用传统 HTTP 请求的方式去发送请求。至于之后是否需要推送消息给用户，由服务端调用 jike-io 的接口进行实现。
在客户端层面，每一个在线用户都会向服务端建立一个 websocekt 连接，后端会为每一个用户建立一个单独的 room 。所有需要通知到该用户的消息都会使用该 websocket 连接进行推送。这种设计相较于针对不同场景建立不同的room的方案，带来的好处是无论需求如何变化，我们的 room 数目永远是和在线用户数一致了，避免个别复杂需求导致 room 数目暴涨。而具体消息类型我们通过自己定义数据格式来进行鉴别。
我们的整套方案是完全依赖于 socket.io 的，期间也遇到了不少大大小小的坑。有些其实是我们自己的场景与其设计初衷不是非常吻合导致的，还有一些算是它的设计缺陷。
在讨论上述问题之前，我们需要去弄明白的一个事情是，socket.io 到底背后做了哪些事情。
socket.io 的设计与实现细节 socket.io 是什么 socket.io 是一个非常流行的实时通信框架, 在 Github 上已经积累了 43574 个 star 。它在开源软件里可以算是一个非常产品化了的软件。拥有相对良好的生态，对于许多功能的封装也很体贴，从移动端到 Web 再到服务端都有比较完整的实现。
socket.io 并不等同于 websocket 框架，它在运行平台不支持 websocket 的时候能够自动回退到 long poll 的方式建立实时通信。同时也实现了断线重连机制。还封装了一套 namespace &amp;amp;&amp;amp; room 的代码层概念。在分布式方面，socket.io 支持多种 adapter 作为 backend 。话虽这么说，但目前看上去可以用的且被人广泛使用的也只有 redis 作为 backend 的 adapter 。所以以下讨论建立在 socket.io-redis 基础之上。</description></item><item><title>使用 Surge 提升多网络环境下的流畅开发体验</title><link>https://blog.joway.io/posts/surge-network/</link><pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/surge-network/</guid><description>作为一名后端工程师经常需要在各种网络环境中切换，由于网络拓扑本身的复杂性以及一些网络工具之间的冲突和bug，常常会在切换中造成不必要的麻烦和痛苦。通常很容易在工作中听到同事会问这些问题 :
你有开 vpn 吗 ? 你开了 ss 了吗 ? 你有同时开 ss 和 vpn 吗 ? 你 http 代理是不是被顶掉了 ? 如果同是技术同事间交流那可能还容易，如果是技术和非技术间交流网络情况，那简直是一个灾难。
而事实上，在绝大部份时候，我们对于网络拓扑的需求是可被精确描述的，也就是说理想情况下不应当存在一个我为了访问某个服务而手动选择要进入某个网络环境的事情。
这篇文章会介绍我们在构建复杂网络环境中的良好开发体验时踩过的坑以及最终是如何优雅地解决这个问题的过程。
历史方案演变 常见的网络环境有:
正常大陆网络 能够上国外网站的网络 公司内网 各个服务器集群的内网 如果你自己还折腾了一些服务器或者家庭网络，那可能还会更加复杂。
之前摸索出的一套还算比较方便的解决方案是 :
在本地常驻一个 ss client 并开放 http 代理端口 在浏览器上使用 Proxy SwitchyOmega 使 Chrome 都走 ss client 的 http 代理 开一个 openvpn 连接到服务器内部网络 这种配置方式能够使得我既能连接所有服务器线上服务和数据库，也能自由地用浏览器去 Google 查一些资料。缺点是丢失了办公室原本的网络环境，另外如果你们服务器有两个完全隔离的子网，那么你可能需要同时连两个 vpn 。而且还有一个不好的是，你的所有非线上服务访问都经过了线上vpn机器的一层代理，让你的访问速度变慢了不说，对服务器也不是一个好事。此外，如果你的一些软件无法手动配置代理，那他们只能默认走 vpn 的网络，对于一些需要访问国外服务器的软件来说就麻烦了。
基于以上缺点，我们又迭代出了另外一个方案:
在服务器上安装一个 ss server 在本地常驻两个 ss client ，一个指向生产服务器 ss, 一个指向国外 ss , 并开放 socks5 代理端口 使用 Proxifier 代理所有本机连接并指定一些规则选择直接访问还是转发到本地的两个 ss client 上。例如我选择让所有 10.</description></item><item><title>一份其实好吃的 LaTeX 入门餐</title><link>https://blog.joway.io/posts/latex/</link><pubDate>Sun, 13 May 2018 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/latex/</guid><description>最近在使用 LaTeX 写作，发现虽然这个「软件」使用简单，设计简约，但使用起来却并不是非常的容易，加上其生态非常芜杂，各种宏包和发行版层出不穷，中文世界鲜有文章系统地去讲他们之间的关系。这篇文章不会去介绍其基本用法，而是以一个更为宏观的角度，旨在厘清 TeX 排版系统的来龙去脉，以及其生态圈中各个项目的作用与关系。或有纰漏，还望雅正。
标题致敬 Liam Huang 老师很流行的一篇文章 《一份其实很短的 LaTeX 入门文档》 。
什么是 Tex TeX 是高德纳教授在70年代末编写 The Art of Computer Programming 时，对当时的计算机排版技术感到无法忍受，因而决定自己开发一个高质量的计算机排版系统 TeX 。
TeX 的版本号有别于当下流行的 x.x.x，而是以圆周率 π 的形式。当前的版本号是 3.14159265 ，所以下一个版本是 3.141592653 。最终无限收敛到 π ，代表了 TeX 不断追求完美的理想。而事实上 TeX 也的确堪称「完美」，高德纳甚至曾悬赏任何发现 Bug 的人，每一个漏洞的奖励金额从2.56美元开始，之后每发现一个 Bug 都会翻倍，直至327.68美元封顶。
TeX 的输出文件被称为 DVI(Device Independent) 文件，DVI 可以作为一种界面描述的中间格式，通过它可以再进而转换成 PDF 格式。
为了区分概念，我们应当将高德纳写的 TeX 软件分为 TeX 语法 和 TeX 编译器。虽然高德纳自己写了一个 TeX 编译器，但其它人依旧可以在不同平台自己编程实现 TeX 语法的编译器。为了保持语法的稳定，TeX 有一组严格的测试文件，如果测试文件文件的输出结果不同于预定的结果，那么这个排版系统就不能够被称为「TeX」。这些不同的 TeX 编译器我们都称之为 「 TeX 引擎」。</description></item><item><title>Kafka 的设计与实践思考</title><link>https://blog.joway.io/posts/kafka-design-practice/</link><pubDate>Mon, 16 Apr 2018 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/kafka-design-practice/</guid><description>前几天看了 librdkafka 的官方文档，这篇文档不仅仅讲解了如何使用 Kafka ，某种程度也讲解了分布式系统实现的难点和使用细节，故而让我对 Kafka 的实现原理产生了浓厚的兴趣。
这篇文章从 Kafka 的设计到使用做了一些个人总结，围绕真正实践场景，探寻其设计上的智慧与妥协。
设计 架构设计 Zookeeper Zookeeper 存储了 Kafka 集群状态信息 。
Zookeeper 还负责从 Broker 中选举出一个机器作为 Controller, 并确保其唯一性。 同时, 当 Controller 宕机时, 再选举一个新的 。
在 0.9 版本之前，它还存储着 Consumer 的 offset 信息 。
Broker 接收 Producer 和 Consumer 的请求，并把 Message 持久化到本地磁盘。
集群会经由 ZK 选举出一个 Broker 来担任 Controller，负责处理各个 Partition 的 Leader 选举，协调 Partition 迁移等工作。
内部组件设计 Topic 逻辑概念，一个 Topic 的数据会被划分到一个或多个 Partition 中。
Partition 最小分配单位。一个 Partition 对应一个目录，该目录可以被单独挂在到一个磁盘上，以实现IO压力的负载均衡。同时多个 Partition 分布在多台机器上，也实现了灵活地水平扩容。</description></item><item><title>Lemon : Koa 风格的 Python 异步 Web 框架</title><link>https://blog.joway.io/posts/lemon-overview/</link><pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/lemon-overview/</guid><description>前段时间想要写一些简短高效的 API ，背后的工作无非就是一些计算和数据处理，但是可能并发量会比较高。当我在 Python 的生态里去搜寻一些靠谱的 Web 框架时，很难找到一个设计优秀且运行效率高的框架。尤其是当我已经习惯了 NodeJS 的 Koa 那种简洁明了的设计时，很难再喜欢上像 Flask 那种装饰器的写法和各种概念拢杂在一起的设计。最后实在没有办法，就自己写了个符合我个人审美的框架 Lemon 。
什么是 Web 框架 在讲 Lemon 的设计前，我们先来看一看一个请求是如何被响应的 ，以及框架在其中的作用是什么 :
当一个请求从客户端进入服务器，首先以 TCP 包的形式被接收到，这个时候我们需要手动去建立连接，接收到 TCP 报文后我们需要去判断它的协议，然后再解成相应协议(一般都是HTTP协议)的报文，传给 application 去处理，当处理完后还要把结果写回成 TCP 报文传回去，如果有 keep alive 还需要去手动管理这个连接的生命周期。以上这些统称为 server 部分。
而和开发者关系最密切的 application 部分做的就是拿到一个 http 或其它协议的报文，然后根据其信息做针对性的响应，传给 server 。
无论你是使用任何语言任何框架，都逃不开上面这个处理过程。而我们在比较框架的时候，无外乎是对以下几个方面做一些针对性的优化:
TCP报文解析成 HTTP(或其它协议) 报文的效率 并发策略 (多线程，多进程，协程，线程池，Event Loop) application 本身的运行效率 (由框架的效率，所用的语言和使用者自身的代码质量共同决定) 对于第一点，python有一个叫 httptools 的库就是使用了 NodeJS 的 http parser 以达到高性能解包的目的。
针对第二点，有许多OS层面和工程层面的技术在致力于解决这个问题。uvloop 就是其中的一种，采用事件循环的方式，底层用的是 libuv , 同样也是 NodeJS 的底层异步IO库 。</description></item><item><title>Golang : Make Programming Happy Again</title><link>https://blog.joway.io/posts/golang-talk/</link><pubDate>Sat, 30 Dec 2017 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/golang-talk/</guid><description>之前在公司内部做技术分享写的一个关于 Golang 的 slide ，花费了挺多的时间的，所以就脱敏了发出来。个人觉得值得一看 ，至少 PPT 的设计很不错 :) 。
Google Doc 地址 :
https://docs.google.com/presentation/d/1odpCfHE5dp7acgK_7lkqTPgHlvT-jPg-XtM7-s9WQZw/edit?usp=sharing</description></item><item><title>Kubernetes 中使用 API Gateway 替代 Ingress</title><link>https://blog.joway.io/posts/kubernetes-gateway/</link><pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/kubernetes-gateway/</guid><description>背景 最近在构思基于 Kubernetes 建立一个个人的开放云平台 , 听起来有点不自量力 , 不过作为个人业余小玩意还是蛮好玩的。最终的成品希望是用户能够轻松地在平台上跑一些简单的无状态服务 和 cronjob 。
在搭建平台的时候遇到的第一个困难是需要有一个好用且功能全面的 API Gateway , 主流的网关服务大多是基于 OpenResty 基础上进行二次开发 , 所需要完成的工作无非是负载均衡，和 API 管理, 加上一些零零碎碎的小功能。
负载均衡 负载均衡分为四层和七层两种 , 以大家所熟知的 Nginx 为例 , 在它的 conf 文件中 , 有 http {} 和 stream {} 两种 block 。
# 四层负载均衡 stream { server { listen 80; proxy_pass app; } upstream app { server 172.31.0.1:8000; } } # 七层负载均衡 http { upstream app { server 192.168.0.1:8000; server 192.</description></item><item><title>ElasticSearch 最佳实践</title><link>https://blog.joway.io/posts/elasticsearch-bp/</link><pubDate>Sun, 28 May 2017 00:00:00 +0000</pubDate><guid>https://blog.joway.io/posts/elasticsearch-bp/</guid><description>Elasticsearch 是一个需要不停调参数的庞然大物 , 从其自身的设置到JVM层面, 有着无数的参数需要根据业务的变化进行调整。最近采用3台 AWS r3.2xlarge , 32GB, 4核, 构建了一套日均日志量过亿的 EFK 套件。经过不停地查阅文档进行调整优化 , 目前日常CPU占用只在30% , 大部分 Kibana 内的查询都能在 5s ~ 15s 内完成。
下面记录了一些实践过程中积累的经验。
硬件 CPU 多核胜过高性能单核CPU 实践中发现, 在高写入低查询的场景下, 日常状态时 , CPU 还能基本应付, 一旦进行 kibana 上的查询或者 force merge 时, CPU 会瞬间飙高, 从而导致写入变慢, 需要很长一段时间 cpu 才能降下来。 Mem Elasticsearch 需要使用大量的堆内存, 而 Lucene 则需要消耗大量非堆内存 (off-heap)。推荐给 ES 设置本机内存的一半, 如32G 内存的机器上, 设置 -Xmx16g -Xms16g ，剩下的内存给 Lucene 。 如果你不需要对分词字符串做聚合计算（例如，不需要 fielddata ）可以考虑降低堆内存。堆内存越小，Elasticsearch（更快的 GC）和 Lucene（更多的内存用于缓存）的性能越好。 由于 JVM 的一些机制 , 内存并不是越大越好, 推荐最大只设置到 31 GB 。 禁用 swap sudo swapoff -a 配置 PS: 应该尽可能使用 ansible 这类工具去管理集群 , 否则集群内机器的状态不一致将是一场噩梦。</description></item></channel></rss>